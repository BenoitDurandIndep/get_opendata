{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import openaq \n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "DATA_PATH=\"C:\\\\Projets\\\\WeatherData\\\\POC\\\\\"\n",
    "\n",
    "# load API keys from a file in folder .env, a line with the key name and the key value separated by a = sign\n",
    "def load_api_keys():\n",
    "    api_keys = {}\n",
    "    with open(\".env\\\\API_KEYS.cfg\") as f:\n",
    "        for line in f:\n",
    "            key, val = line.split(\"=\")\n",
    "            key = key.strip()\n",
    "            val = val.strip()            \n",
    "            api_keys[key] = val\n",
    "    return api_keys\n",
    "\n",
    "api_keys = load_api_keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free API for geocoding : \n",
    "https://openweathermap.org/api/geocoding-api  \n",
    "https://overpass-api.de/\n",
    "\n",
    "https://geocode.maps.co/ >> https://geocode.maps.co/search?street=555+5th+Ave&city=New+York&state=NY&postalcode=10017&country=US&api_key=api_key\n",
    "1 req / sec max 100k / month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bordeaux, Gironde, Nouvelle-Aquitaine, Metropolitan France, France : lat=44.841225, lon=-0.5800364, importance=0.8740050666982948\n"
     ]
    }
   ],
   "source": [
    "# geocoding with geocode.maps.co\n",
    "city=\"BORDEAUX\"\n",
    "country=\"FR\"\n",
    "\n",
    "url_geo=\"https://geocode.maps.co/search?city=\"+city+\"&country=\"+country+\"&api_key=\"+api_keys[\"GEOCODE_MAPS_CO\"]\n",
    "\n",
    "response = requests.get(url_geo)\n",
    "# get first element in response\n",
    "geo_data = response.json()[0]\n",
    "# get display_name, lat, lon and importance\n",
    "display_name = geo_data[\"display_name\"]\n",
    "lat = geo_data[\"lat\"]\n",
    "lon = geo_data[\"lon\"]\n",
    "importance = geo_data[\"importance\"]\n",
    "\n",
    "print(f\"{display_name} : lat={lat}, lon={lon}, importance={importance}\")\n",
    "if importance<0.5:\n",
    "    print(\"Warning: importance is low, the location may be incorrect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get air quality whith https://openaq.org/\n",
    "1-get station : https://api.openaq.org/v2/locations?\n",
    "2- get measurements : https://api.openaq.org/v2/measurements\n",
    "\n",
    "and write in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius=\"20000\"\n",
    "\n",
    "url_rech = \"https://api.openaq.org/v2/locations?country=\"+country+\"&coordinates=\"+lat+\",\"+lon+\"&radius=\"+radius+\"&limit=20&page=1&offset=0&sort=asc&order_by=city\"\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url_rech, headers=headers)\n",
    "\n",
    "json_file_path = DATA_PATH +city+ \"_response.json\"\n",
    "\n",
    "# Write the response.text to the JSON file\n",
    "with open(json_file_path, \"w\",encoding='utf-8') as file:\n",
    "    # json.dump(response.text, file,ensure_ascii=False,indent=4)\n",
    "    file.write(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse the response to build a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                     city country              firstUpdated  \\\n",
      "0  3572  ATMO NOUVELLE-AQUITAINE      FR 2016-11-21 11:00:00+00:00   \n",
      "\n",
      "                 lastUpdated  measurements  \\\n",
      "0  2024-03-04T07:00:00+00:00        211132   \n",
      "\n",
      "                                          parameters   latitude  longitude  \n",
      "0  [{'id': 1, 'unit': 'µg/m³', 'count': 44602, 'a...  44.900276  -0.514722  \n"
     ]
    }
   ],
   "source": [
    "response_json = response.json()\n",
    "results = response_json[\"results\"]\n",
    "\n",
    "# create a DataFrame with interesting columns \n",
    "df = pd.DataFrame(results, columns=[\"id\", \"city\", \"country\", \"firstUpdated\", \"lastUpdated\", \"measurements\", \"coordinates\", \"parameters\"])\n",
    "df[\"latitude\"] = df[\"coordinates\"].apply(lambda x: x[\"latitude\"])\n",
    "df[\"longitude\"] = df[\"coordinates\"].apply(lambda x: x[\"longitude\"])\n",
    "df = df.drop(columns=[\"coordinates\"])\n",
    "\n",
    "#  filter dataframe to get  the oldest firstUpdated and the biggest measurements\n",
    "df[\"firstUpdated\"] = pd.to_datetime(df[\"firstUpdated\"])\n",
    "df = df.sort_values(by=[\"firstUpdated\",\"measurements\"], ascending=[True,False],)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop(df.index[1:])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call averages and loop with the pagination and write responses in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.openaq.org/v2/averages?temporal=day&date_from=2024-01-01T00:00:00Z&date_to=2024-04-30T00:00:00Z&locations_id=3572&spatial=location&limit=200&page=1\n",
      "https://api.openaq.org/v2/averages?temporal=day&date_from=2024-01-01T00:00:00Z&date_to=2024-04-30T00:00:00Z&locations_id=3572&spatial=location&limit=200&page=2\n",
      "https://api.openaq.org/v2/averages?temporal=day&date_from=2024-01-01T00:00:00Z&date_to=2024-04-30T00:00:00Z&locations_id=3572&spatial=location&limit=200&page=3\n",
      "Total results: 204\n"
     ]
    }
   ],
   "source": [
    "location_id=str(df[\"id\"][0])\n",
    "date_from=\"2024-01-01T00:00:00Z\"\n",
    "date_to=\"2024-04-30T00:00:00Z\"\n",
    "limit=\"200\"\n",
    "# warning if more than 1 year timeout\n",
    "\n",
    "# url_rech = \"https://api.openaq.org/v2/measurements?date_from=2023-01-01T00%3A00%3A00Z&date_to=2024-04-30T00%3A00%3A00Z&location_id=3572&limit=100&page=1&offset=0&sort=desc&order_by=datetime\"\n",
    "# url_rech = \"https://api.openaq.org/v2/averages?temporal=day&date_from=\"+date_from+\"&date_to=\"+date_to+\"&locations_id=\"+location_id+\"&spatial=location&limit=\"+limit+\"&page=1&offset=0&sort=desc&order_by=datetime\"\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "page = 1\n",
    "more=True\n",
    "\n",
    "# Create an empty list to store the results\n",
    "all_results = []\n",
    "\n",
    "while more:\n",
    "    # Construct the API URL with the current page and limit\n",
    "    url_rech = f\"https://api.openaq.org/v2/averages?temporal=day&date_from={date_from}&date_to={date_to}&locations_id={location_id}&spatial=location&limit={limit}&page={page}\"\n",
    "    print(url_rech)\n",
    "    # Send the request to the API\n",
    "    response = requests.get(url_rech)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the JSON data from the response\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the results from the JSON data\n",
    "        results = data[\"results\"]\n",
    "\n",
    "        # Append the results to the list\n",
    "        all_results.extend(results)\n",
    "\n",
    "        # Check if there are more pages\n",
    "        if len(results)>0:\n",
    "            # Increment the page number\n",
    "            page += 1\n",
    "            # sleep 1 second to avoid being blocked\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            # Break the loop if there are no more pages\n",
    "            more=False\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "# Print the total number of results\n",
    "print(f\"Total results: {len(all_results)}\")\n",
    "\n",
    "# response = requests.get(url_rech, headers=headers)\n",
    "\n",
    "json_file_path = DATA_PATH +city+ \"_avg_bdx_response.json\"\n",
    "\n",
    "with open(json_file_path, \"w\",encoding='utf-8') as file:\n",
    "    # json.dump(response.text, file,ensure_ascii=False,indent=4)\n",
    "    # file.write(response.text)\n",
    "    file.write(json.dumps(all_results,ensure_ascii=False,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same but with the Python wrapper OpenAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value coordinates period.label period.interval   period.datetime_from.utc  \\\n",
      "0    7.3        None        1hour        01:00:00  2023-12-31T23:00:00+00:00   \n",
      "1   15.0        None        1hour        01:00:00  2023-12-31T23:00:00+00:00   \n",
      "2   22.0        None        1hour        01:00:00  2023-12-31T23:00:00+00:00   \n",
      "3   61.0        None        1hour        01:00:00  2023-12-31T23:00:00+00:00   \n",
      "4    0.4        None        1hour        01:00:00  2023-12-31T23:00:00+00:00   \n",
      "\n",
      "  period.datetime_from.local     period.datetime_to.utc  \\\n",
      "0  2024-01-01T00:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "1  2024-01-01T00:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "2  2024-01-01T00:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "3  2024-01-01T00:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "4  2024-01-01T00:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "\n",
      "    period.datetime_to.local  parameter.id parameter.name  ...  \\\n",
      "0  2024-01-01T01:00:00+01:00             5            no2  ...   \n",
      "1  2024-01-01T01:00:00+01:00             2           pm25  ...   \n",
      "2  2024-01-01T01:00:00+01:00             1           pm10  ...   \n",
      "3  2024-01-01T01:00:00+01:00             3             o3  ...   \n",
      "4  2024-01-01T01:00:00+01:00         19843             no  ...   \n",
      "\n",
      "  coverage.expected_count coverage.expected_interval  coverage.observed_count  \\\n",
      "0                       1                   01:00:00                        1   \n",
      "1                       1                   01:00:00                        1   \n",
      "2                       1                   01:00:00                        1   \n",
      "3                       1                   01:00:00                        1   \n",
      "4                       1                   01:00:00                        1   \n",
      "\n",
      "   coverage.observed_interval  coverage.percent_complete  \\\n",
      "0                    01:00:00                      100.0   \n",
      "1                    01:00:00                      100.0   \n",
      "2                    01:00:00                      100.0   \n",
      "3                    01:00:00                      100.0   \n",
      "4                    01:00:00                      100.0   \n",
      "\n",
      "   coverage.percent_coverage  coverage.datetime_from.utc  \\\n",
      "0                      100.0   2024-01-01T00:00:00+00:00   \n",
      "1                      100.0   2024-01-01T00:00:00+00:00   \n",
      "2                      100.0   2024-01-01T00:00:00+00:00   \n",
      "3                      100.0   2024-01-01T00:00:00+00:00   \n",
      "4                      100.0   2024-01-01T00:00:00+00:00   \n",
      "\n",
      "   coverage.datetime_from.local   coverage.datetime_to.utc  \\\n",
      "0     2024-01-01T01:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "1     2024-01-01T01:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "2     2024-01-01T01:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "3     2024-01-01T01:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "4     2024-01-01T01:00:00+01:00  2024-01-01T00:00:00+00:00   \n",
      "\n",
      "  coverage.datetime_to.local  \n",
      "0  2024-01-01T01:00:00+01:00  \n",
      "1  2024-01-01T01:00:00+01:00  \n",
      "2  2024-01-01T01:00:00+01:00  \n",
      "3  2024-01-01T01:00:00+01:00  \n",
      "4  2024-01-01T01:00:00+01:00  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "openaq_key=api_keys[\"OPENAQ\"]\n",
    "location_id=str(df[\"id\"][0])\n",
    "date_from=\"2024-01-01\"\n",
    "date_to=\"2024-04-30\"\n",
    "\n",
    "client = OpenAQ(api_key=openaq_key)\n",
    "response = client.measurements.list(locations_id=location_id, date_from=date_from, date_to=date_to)\n",
    "data_dict = response.dict()\n",
    "df = pd.json_normalize(data_dict['results'])\n",
    "client.close()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from NOAA\n",
    "Bordeaux station: FR000007510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         station_id  latitude  longitude  elevation state  \\\n",
      "0       ACW00011604   17.1167   -61.7833       10.1   NaN   \n",
      "1       ACW00011647   17.1333   -61.7833       19.2   NaN   \n",
      "2       AE000041196   25.3330    55.5170       34.0   NaN   \n",
      "3       AEM00041194   25.2550    55.3640       10.4   NaN   \n",
      "4       AEM00041217   24.4330    54.6510       26.8   NaN   \n",
      "...             ...       ...        ...        ...   ...   \n",
      "125983  ZI000067969  -21.0500    29.3670      861.0   NaN   \n",
      "125984  ZI000067975  -20.0670    30.8670     1095.0   NaN   \n",
      "125985  ZI000067977  -21.0170    31.5830      430.0   NaN   \n",
      "125986  ZI000067983  -20.2000    32.6160     1132.0   NaN   \n",
      "125987  ZI000067991  -22.2170    30.0000      457.0   NaN   \n",
      "\n",
      "                         name gsn_flag hcn_crn_flag   wmo_id  \n",
      "0       ST JOHNS COOLIDGE FLD      NaN          NaN      NaN  \n",
      "1                    ST JOHNS      NaN          NaN      NaN  \n",
      "2         SHARJAH INTER. AIRP      GSN          NaN  41196.0  \n",
      "3                  DUBAI INTL      NaN          NaN  41194.0  \n",
      "4              ABU DHABI INTL      NaN          NaN  41217.0  \n",
      "...                       ...      ...          ...      ...  \n",
      "125983         WEST NICHOLSON      NaN          NaN  67969.0  \n",
      "125984               MASVINGO      NaN          NaN  67975.0  \n",
      "125985          BUFFALO RANGE      NaN          NaN  67977.0  \n",
      "125986               CHIPINGE      GSN          NaN  67983.0  \n",
      "125987             BEITBRIDGE      NaN          NaN  67991.0  \n",
      "\n",
      "[125988 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "file_station=\"ghcnd-stations.txt\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_station = \"ghcnd-stations.txt\"\n",
    "df_stations = pd.read_fwf(DATA_PATH+file_station, header=None,widths=[11,9,10,7,4,31,4,4,5], names=[\"station_id\", \"latitude\", \"longitude\", \"elevation\", \"state\", \"name\", \"gsn_flag\", \"hcn_crn_flag\", \"wmo_id\"])\n",
    "print(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest city to city='BORDEAUX' country='FR' is BORDEAUX-MERIGNAC : station_id=FR000007510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projets\\get_opendata\\.env\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "# TODO\n",
    "# UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
    "#   warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
    "\n",
    "city=\"BORDEAUX\"\n",
    "country=\"FR\"\n",
    "\n",
    "\n",
    "# Filter df_stations by station_id starting with country\n",
    "filtered_stations = df_stations[df_stations['station_id'].str.startswith(country)].copy()\n",
    "\n",
    "# Calculate the Levenshtein distance between each city name and the country variable\n",
    "filtered_stations['distance'] = filtered_stations['name'].apply(lambda x: fuzz.ratio(x, city))\n",
    "\n",
    "# # Find the city name with the minimum distance\n",
    "closest_city = filtered_stations.loc[filtered_stations['distance'].idxmax()]\n",
    "\n",
    "print(f\"The closest city to {city=} {country=} is {closest_city['name']} : station_id={closest_city['station_id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from the closest city\n",
    "station_id = closest_city['station_id']\n",
    "today_YYYYMMDD=time.strftime(\"%Y%m%d\")\n",
    "local_file_name=f\"{station_id}_{today_YYYYMMDD}.csv.gz\"\n",
    "\n",
    "url_by_station = f\"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_station/{station_id}.csv.gz\"\n",
    "url_all = f\"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all/{station_id}.dly\"\n",
    "\n",
    "# Download the data from the station and write it to a local file\n",
    "response = requests.get(url_by_station) \n",
    "with open(DATA_PATH+local_file_name, 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       station_id       date element  value m_flag q_flag s_flag  obs_time\n",
      "1000  FR000007510 1903-09-28    PRCP     40    NaN    NaN      I       NaN\n",
      "1001  FR000007510 1903-09-29    PRCP      0    NaN    NaN      I       NaN\n",
      "1002  FR000007510 1903-09-30    PRCP      0    NaN    NaN      I       NaN\n",
      "1003  FR000007510 1903-10-01    PRCP     70    NaN    NaN      I       NaN\n",
      "1004  FR000007510 1903-10-02    PRCP     30    NaN    NaN      I       NaN\n"
     ]
    }
   ],
   "source": [
    "# open the local file and load into a DataFrame\n",
    "df = pd.read_csv(DATA_PATH+local_file_name, header=None, names=[\"station_id\", \"date\", \"element\", \"value\", \"m_flag\", \"q_flag\", \"s_flag\", \"obs_time\"], parse_dates=[\"date\"])\n",
    "print(df[1000:1005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAME FOR PHUKET without useless code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phuket, Ratsada, Mueang Phuket, Phuket Province, 83000, Thailand : lat=7.8847901, lon=98.3891503, importance=0.6309396794654166\n",
      "        id    city country              firstUpdated  \\\n",
      "0  1236044    None      TH 2024-06-13 21:00:00+00:00   \n",
      "1  2827393    None      TH 2024-06-13 21:00:00+00:00   \n",
      "2  2827392    None      TH 2024-06-13 21:00:00+00:00   \n",
      "3   225605  Phuket      TH 2024-06-13 22:00:00+00:00   \n",
      "4  2926351    None      TH 2024-07-01 12:00:00+00:00   \n",
      "\n",
      "                 lastUpdated  measurements  \\\n",
      "0  2024-07-22T09:00:00+00:00          4602   \n",
      "1  2024-07-22T09:00:00+00:00          2427   \n",
      "2  2024-06-15T12:00:00+00:00           120   \n",
      "3  2024-07-22T08:00:00+00:00          1227   \n",
      "4  2024-07-22T09:00:00+00:00          2700   \n",
      "\n",
      "                                          parameters  latitude  longitude  \n",
      "0  [{'id': 100, 'unit': 'c', 'count': 767, 'avera...  8.028639  98.354412  \n",
      "1  [{'id': 100, 'unit': 'c', 'count': 809, 'avera...  7.913188  98.385946  \n",
      "2  [{'id': 100, 'unit': 'c', 'count': 40, 'averag...  7.909639  98.386987  \n",
      "3  [{'id': 8, 'unit': 'ppm', 'count': 18, 'averag...  7.884488  98.391283  \n",
      "4  [{'id': 125, 'unit': 'particles/cm³', 'count':...  7.860515  98.369035  \n",
      "    value coordinates period.label period.interval   period.datetime_from.utc  \\\n",
      "10   68.0        None        1hour        01:00:00  2024-03-31T18:00:00+00:00   \n",
      "11   18.0        None        1hour        01:00:00  2024-03-31T18:00:00+00:00   \n",
      "12   17.0        None        1hour        01:00:00  2024-03-31T19:00:00+00:00   \n",
      "13   27.0        None        1hour        01:00:00  2024-03-31T19:00:00+00:00   \n",
      "14   68.0        None        1hour        01:00:00  2024-03-31T19:00:00+00:00   \n",
      "\n",
      "   period.datetime_from.local     period.datetime_to.utc  \\\n",
      "10  2024-04-01T01:00:00+07:00  2024-03-31T19:00:00+00:00   \n",
      "11  2024-04-01T01:00:00+07:00  2024-03-31T19:00:00+00:00   \n",
      "12  2024-04-01T02:00:00+07:00  2024-03-31T20:00:00+00:00   \n",
      "13  2024-04-01T02:00:00+07:00  2024-03-31T20:00:00+00:00   \n",
      "14  2024-04-01T02:00:00+07:00  2024-03-31T20:00:00+00:00   \n",
      "\n",
      "     period.datetime_to.local  parameter.id    parameter.name  ...  \\\n",
      "10  2024-04-01T02:00:00+07:00            98  relativehumidity  ...   \n",
      "11  2024-04-01T02:00:00+07:00             2              pm25  ...   \n",
      "12  2024-04-01T03:00:00+07:00             1              pm10  ...   \n",
      "13  2024-04-01T03:00:00+07:00           100       temperature  ...   \n",
      "14  2024-04-01T03:00:00+07:00            98  relativehumidity  ...   \n",
      "\n",
      "   coverage.expected_count coverage.expected_interval  \\\n",
      "10                       1                   01:00:00   \n",
      "11                       1                   01:00:00   \n",
      "12                       1                   01:00:00   \n",
      "13                       1                   01:00:00   \n",
      "14                       1                   01:00:00   \n",
      "\n",
      "    coverage.observed_count  coverage.observed_interval  \\\n",
      "10                        1                    01:00:00   \n",
      "11                        1                    01:00:00   \n",
      "12                        1                    01:00:00   \n",
      "13                        1                    01:00:00   \n",
      "14                        1                    01:00:00   \n",
      "\n",
      "    coverage.percent_complete  coverage.percent_coverage  \\\n",
      "10                      100.0                      100.0   \n",
      "11                      100.0                      100.0   \n",
      "12                      100.0                      100.0   \n",
      "13                      100.0                      100.0   \n",
      "14                      100.0                      100.0   \n",
      "\n",
      "    coverage.datetime_from.utc  coverage.datetime_from.local  \\\n",
      "10   2024-03-31T19:00:00+00:00     2024-04-01T02:00:00+07:00   \n",
      "11   2024-03-31T19:00:00+00:00     2024-04-01T02:00:00+07:00   \n",
      "12   2024-03-31T20:00:00+00:00     2024-04-01T03:00:00+07:00   \n",
      "13   2024-03-31T20:00:00+00:00     2024-04-01T03:00:00+07:00   \n",
      "14   2024-03-31T20:00:00+00:00     2024-04-01T03:00:00+07:00   \n",
      "\n",
      "     coverage.datetime_to.utc coverage.datetime_to.local  \n",
      "10  2024-03-31T19:00:00+00:00  2024-04-01T02:00:00+07:00  \n",
      "11  2024-03-31T19:00:00+00:00  2024-04-01T02:00:00+07:00  \n",
      "12  2024-03-31T20:00:00+00:00  2024-04-01T03:00:00+07:00  \n",
      "13  2024-03-31T20:00:00+00:00  2024-04-01T03:00:00+07:00  \n",
      "14  2024-03-31T20:00:00+00:00  2024-04-01T03:00:00+07:00  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# geocoding with geocode.maps.co\n",
    "city=\"PHUKET\"\n",
    "country=\"TH\"\n",
    "\n",
    "url_geo=\"https://geocode.maps.co/search?city=\"+city+\"&country=\"+country+\"&api_key=\"+api_keys[\"GEOCODE_MAPS_CO\"]\n",
    "\n",
    "response = requests.get(url_geo)\n",
    "# get first element in response\n",
    "geo_data = response.json()[0]\n",
    "# get display_name, lat, lon and importance\n",
    "display_name = geo_data[\"display_name\"]\n",
    "lat = geo_data[\"lat\"]\n",
    "lon = geo_data[\"lon\"]\n",
    "importance = geo_data[\"importance\"]\n",
    "\n",
    "print(f\"{display_name} : lat={lat}, lon={lon}, importance={importance}\")\n",
    "if importance<0.5:\n",
    "    print(\"Warning: importance is low, the location may be incorrect\")\n",
    "\n",
    "radius=\"20000\"\n",
    "\n",
    "url_rech = \"https://api.openaq.org/v2/locations?country=\"+country+\"&coordinates=\"+lat+\",\"+lon+\"&radius=\"+radius+\"&limit=20&page=1&offset=0&sort=asc&order_by=city\"\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url_rech, headers=headers)\n",
    "\n",
    "json_file_path = DATA_PATH +city+ \"_response.json\"\n",
    "\n",
    "# Write the response.text to the JSON file\n",
    "with open(json_file_path, \"w\",encoding='utf-8') as file:\n",
    "    # json.dump(response.text, file,ensure_ascii=False,indent=4)\n",
    "    file.write(response.text)\n",
    "\n",
    "response_json = response.json()\n",
    "results = response_json[\"results\"]\n",
    "\n",
    "# create a DataFrame with interesting columns \n",
    "df_aq = pd.DataFrame(results, columns=[\"id\", \"city\", \"country\", \"firstUpdated\", \"lastUpdated\", \"measurements\", \"coordinates\", \"parameters\"])\n",
    "df_aq[\"latitude\"] = df_aq[\"coordinates\"].apply(lambda x: x[\"latitude\"])\n",
    "df_aq[\"longitude\"] = df_aq[\"coordinates\"].apply(lambda x: x[\"longitude\"])\n",
    "df_aq = df_aq.drop(columns=[\"coordinates\"])\n",
    "\n",
    "#  filter dataframe to get  the oldest firstUpdated and the biggest measurements\n",
    "df_aq[\"firstUpdated\"] = pd.to_datetime(df_aq[\"firstUpdated\"])\n",
    "df_aq = df_aq.sort_values(by=[\"firstUpdated\",\"measurements\"], ascending=[True,False],)\n",
    "df_aq = df_aq.reset_index(drop=True)\n",
    "# df_aq = df_aq.drop(df.index[1:])\n",
    "print(df_aq.head())\n",
    "\n",
    "openaq_key=api_keys[\"OPENAQ\"]\n",
    "location_id=str(df_aq[\"id\"][0])\n",
    "date_from=\"2024-04-01\"\n",
    "date_to=\"2024-04-30\"\n",
    "\n",
    "client = openaq.OpenAQ(api_key=openaq_key)\n",
    "response = client.measurements.list(locations_id=location_id, date_from=date_from, date_to=date_to)\n",
    "data_dict = response.dict()\n",
    "df_aq_res = pd.json_normalize(data_dict['results'])\n",
    "\n",
    "print(df_aq_res[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest city to city='PHUKET' country='TH' is PHUKET : station_id=TH000048564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hebus\\AppData\\Local\\Temp\\ipykernel_9964\\4127213658.py:35: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_PATH+local_file_name, header=None, names=[\"station_id\", \"date\", \"element\", \"value\", \"m_flag\", \"q_flag\", \"s_flag\", \"obs_time\"], parse_dates=[\"date\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       station_id       date element  value m_flag q_flag s_flag  obs_time\n",
      "1000  TH000048564 1952-01-17    TMAX    325    NaN    NaN      I       NaN\n",
      "1001  TH000048564 1952-01-18    TMAX    317    NaN    NaN      I       NaN\n",
      "1002  TH000048564 1952-01-19    TMAX    325    NaN    NaN      I       NaN\n",
      "1003  TH000048564 1952-01-20    TMAX    321    NaN    NaN      I       NaN\n",
      "1004  TH000048564 1952-01-21    TMAX    305    NaN    NaN      I       NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO\n",
    "# UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
    "#   warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
    "\n",
    "city=\"PHUKET\"\n",
    "country=\"TH\"\n",
    "\n",
    "file_station = \"ghcnd-stations.txt\"\n",
    "df_stations = pd.read_fwf(DATA_PATH+file_station, header=None,widths=[11,9,10,7,4,31,4,4,5], names=[\"station_id\", \"latitude\", \"longitude\", \"elevation\", \"state\", \"name\", \"gsn_flag\", \"hcn_crn_flag\", \"wmo_id\"])\n",
    "\n",
    "# Filter df_stations by station_id starting with country\n",
    "filtered_stations = df_stations[df_stations['station_id'].str.startswith(country)].copy()\n",
    "\n",
    "# Calculate the Levenshtein distance between each city name and the country variable\n",
    "filtered_stations['distance'] = filtered_stations['name'].apply(lambda x: fuzz.ratio(x, city))\n",
    "\n",
    "# # Find the city name with the minimum distance\n",
    "closest_city = filtered_stations.loc[filtered_stations['distance'].idxmax()]\n",
    "\n",
    "print(f\"The closest city to {city=} {country=} is {closest_city['name']} : station_id={closest_city['station_id']}\")\n",
    "\n",
    "# Download the data from the closest city\n",
    "station_id = closest_city['station_id']\n",
    "today_YYYYMMDD=time.strftime(\"%Y%m%d\")\n",
    "local_file_name=f\"{station_id}_{today_YYYYMMDD}.csv.gz\"\n",
    "\n",
    "url_by_station = f\"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_station/{station_id}.csv.gz\"\n",
    "url_all = f\"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all/{station_id}.dly\"\n",
    "\n",
    "# Download the data from the station and write it to a local file\n",
    "response = requests.get(url_by_station) \n",
    "with open(DATA_PATH+local_file_name, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH+local_file_name, header=None, names=[\"station_id\", \"date\", \"element\", \"value\", \"m_flag\", \"q_flag\", \"s_flag\", \"obs_time\"], parse_dates=[\"date\"])\n",
    "print(df[1000:1005])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
